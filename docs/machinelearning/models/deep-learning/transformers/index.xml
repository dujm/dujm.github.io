<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Website – Transformers</title>
    <link>/docs/machinelearning/models/deep-learning/transformers/</link>
    <description>Recent content in Transformers on My Website</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 14 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	  <atom:link href="/docs/machinelearning/models/deep-learning/transformers/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Hugging Face</title>
      <link>/docs/machinelearning/models/deep-learning/transformers/hugging-face/</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/docs/machinelearning/models/deep-learning/transformers/hugging-face/</guid>
      <description>
        
        
        &lt;br&gt; 
&lt;br&gt; 
&lt;p&gt;&lt;img src=&#34;https://raw.githubusercontent.com/huggingface/transformers/master/docs/source/imgs/transformers_logo_name.png&#34; alt=&#34;Hugging Face&#34;&gt;&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/transformers&#34;&gt;Transformers&lt;/a&gt;is an opinionated library built for NLP researchers seeking to use/study/extend large-scale transformers models.&lt;/p&gt;
&lt;p&gt;The library was designed with two strong goals in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Be as easy and fast to use as possible:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions, just three standard classes required to use each model: configuration, models and tokenizer,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;all of these classes can be initialized in a simple and unified way from pretrained instances by using a common from_pretrained() instantiation method which will take care of downloading (if needed), caching and loading the related class from a pretrained instance supplied in the library or your own saved instance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;as a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to extend/build-upon the library, just use regular Python/PyTorch modules and inherit from the base classes of the library to reuse functionalities like model loading/saving.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide state-of-the-art models with performances as close as possible to the original models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we provide at least one example for each architecture which reproduces a result provided by the official authors of said architecture,&lt;/li&gt;
&lt;li&gt;the code is usually as close to the original code base as possible which means some PyTorch code may be not as pytorchic as it could be as a result of being converted TensorFlow code.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br&gt;
&lt;h3 id=&#34;three-types-of-classes-for-each-model&#34;&gt;Three types of classes for each model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Model classes e.g., BertModel which are 20+ PyTorch models (torch.nn.Modules) that work with the pretrained weights provided in the library. In TF2, these are tf.keras.Model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configuration classes which store all the parameters required to build a model, e.g., BertConfig. You don’t always need to instantiate these your-self. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tokenizer classes which store the vocabulary for each model and provide methods for encoding/decoding strings in a list of token embeddings indices to be fed to a model, e.g., BertTokenizer&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All these classes can be instantiated from pretrained instances and saved locally using two methods:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;from_pretrained() let you instantiate a model/configuration/tokenizer from a pretrained version either provided by the library itself (currently 27 models are provided as listed here) or stored locally (or on a server) by the user,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;save_pretrained() let you save a model/configuration/tokenizer locally so that it can be reloaded using from_pretrained().&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Benchmark</title>
      <link>/docs/machinelearning/models/deep-learning/transformers/benchmark/</link>
      <pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/docs/machinelearning/models/deep-learning/transformers/benchmark/</guid>
      <description>
        
        
        &lt;br&gt;
&lt;h3 id=&#34;long-range-arena-lra&#34;&gt;Long-Range Arena (LRA)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Long-range arena is an effort toward systematic evaluation of efficient transformer models. The project aims at establishing benchmark tasks/dtasets using which we can evaluate transformer-based models in a systematic way, by assessing their generalization power, computational efficiency, memory foot-print, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Long-range arena also implements different variants of Transformer models in JAX, using Flax.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;This first initial release includes the benchmarks for the paper &lt;a href=&#34;https://arxiv.org/pdf/2011.04006.pdf&#34;&gt;Long Range Arena: A benchmark for Efficient Transformers&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/google-research/long-range-arena&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
